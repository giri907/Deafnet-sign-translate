{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6bec2c",
   "metadata": {},
   "source": [
    "# üß† DeafNet: Real-Time Sign Language Recognition\n",
    "\n",
    "This project implements a Convolutional Neural Network (CNN) to recognize American Sign Language (ASL) gestures from a live webcam feed using Python, TensorFlow/Keras, and OpenCV.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ‚öôÔ∏è Setup and Installation\n",
    "\n",
    "Follow these steps to set up your environment and install the required dependencies.\n",
    "\n",
    "### *1.1. Prerequisites\n",
    "\n",
    "* Python 3.x (Recommended: 3.8+)\n",
    "* A working webcam.\n",
    "\n",
    "### *1.2. Installation Command\n",
    "\n",
    "Use pip to install all necessary libraries, including TensorFlow for the model, OpenCV for video processing, and NumPy.\n",
    "\n",
    "```bash\n",
    "pip install tensorflow opencv-python numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82e6aa",
   "metadata": {},
   "source": [
    "## 2. üìä Data Preparation\n",
    "\n",
    "### 2.1. Load and Preprocess Data\n",
    "\n",
    "Assuming you have image data (e.g., a folder of images or a CSV like Sign MNIST), load it and perform standardization.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Placeholder: Replace this with your actual data loading mechanism\n",
    "# X_data should be images (N, 28, 28, 1) and Y_data should be labels (N,)\n",
    "# X_data, Y_data = load_my_sign_data() \n",
    "\n",
    "# 1. Normalize Pixel Values\n",
    "X_data = X_data.astype('float32') / 255.0\n",
    "\n",
    "# 2. One-Hot Encode Labels\n",
    "num_classes = 26 # For A-Z\n",
    "Y_data = to_categorical(Y_data, num_classes=num_classes)\n",
    "\n",
    "# 3. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, Y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training shapes: X={X_train.shape}, Y={y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed070177",
   "metadata": {},
   "source": [
    "## 3. üß† Model Building and Training\n",
    "\n",
    "### 3.1. Define the CNN Architecture\n",
    "\n",
    "We use a simple Sequential model with Convolutional layers for image feature extraction.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "EPOCHS = 20  # Number of passes over the entire dataset\n",
    "BATCH_SIZE = 64 # Number of samples processed before the model is updated\n",
    "\n",
    "# Start training the model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    # Use the test data (or a separate validation set) to monitor performance during training\n",
    "    validation_data=(X_test, Y_test), \n",
    "    verbose=1 # Display progress during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae491141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297cca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to an HDF5 file\n",
    "# This saves the architecture, weights, and training configuration.\n",
    "model.save('deafnet_sign_recognition_cnn.h5')\n",
    "print(\"\\nModel saved successfully to 'deafnet_sign_recognition_cnn.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93dd435",
   "metadata": {},
   "source": [
    "## 4. ‚ñ∂Ô∏è Real-Time Prediction (Testing)\n",
    "\n",
    "### 4.1. Run Live Detection Logic\n",
    "\n",
    "This code loads the saved model and uses your webcam to recognize signs. *(Note: This requires a working webcam and may be unstable in some notebook environments).*\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('deafnet_slr_model.h5')\n",
    "\n",
    "# Utility to convert index to letter (0=A, 1=B, etc.)\n",
    "def index_to_letter(index):\n",
    "    # This assumes a 0-25 index mapping to A-Z\n",
    "    return chr(65 + index) \n",
    "\n",
    "cap = cv2.VideoCapture(0) # Open the default camera\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 1. Define and draw the Region of Interest (ROI)\n",
    "    # Adjust coordinates based on where you want the user to place their hand\n",
    "    roi_coords = (100, 100, 300, 300) # (x1, y1, x2, y2)\n",
    "    cv2.rectangle(frame, roi_coords[:2], roi_coords[2:], (0, 255, 0), 2)\n",
    "    roi = frame[roi_coords[1]:roi_coords[3], roi_coords[0]:roi_coords[2]]\n",
    "\n",
    "    # 2. Pre-process the ROI\n",
    "    processed_img = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    processed_img = cv2.resize(processed_img, (28, 28))\n",
    "    \n",
    "    # 3. Prepare input for the model\n",
    "    input_data = np.expand_dims(processed_img, axis=[0, -1]).astype('float32') / 255.0\n",
    "    \n",
    "    # 4. Predict\n",
    "    prediction = loaded_model.predict(input_data, verbose=0)\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    predicted_letter = index_to_letter(predicted_index)\n",
    "\n",
    "    # 5. Display Result\n",
    "    cv2.putText(frame, f\"Sign: {predicted_letter}\", (50, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    \n",
    "    cv2.imshow('DeafNet Live Feed', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d43a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- Load the saved model (from Step 2.2) ---\n",
    "MODEL = load_model('deafnet_slr_model.h5')\n",
    "\n",
    "# Define a simple function to map index to letter\n",
    "def get_sign(index):\n",
    "    # This maps 0 to 'A', 1 to 'B', etc.\n",
    "    return chr(65 + index) \n",
    "\n",
    "cap = cv2.VideoCapture(0) # Start webcam capture\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # --- Define ROI (Region of Interest) for the hand ---\n",
    "    # Coordinates for a 200x200 box\n",
    "    x1, y1 = 100, 100\n",
    "    x2, y2 = 300, 300\n",
    "    \n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # --- Pre-process ROI (Grayscale, Resize, Normalize) ---\n",
    "    processed_img = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 1. Resize to the model's expected size (e.g., 64x64)\n",
    "    processed_img = cv2.resize(processed_img, (64, 64)) \n",
    "    \n",
    "    # 2. Add channel dimension for TensorFlow (from (64, 64) to (64, 64, 1))\n",
    "    processed_img = np.expand_dims(processed_img, axis=-1)\n",
    "    \n",
    "    # 3. Normalize the pixel values (0-255 to 0.0-1.0)\n",
    "    processed_img = processed_img / 255.0\n",
    "    \n",
    "    # 4. Add batch dimension (from (64, 64, 1) to (1, 64, 64, 1))\n",
    "    processed_img = np.expand_dims(processed_img, axis=0)\n",
    "    \n",
    "    # --- Prediction ---\n",
    "    predictions = MODEL.predict(processed_img)\n",
    "    predicted_index = np.argmax(predictions)\n",
    "    predicted_sign = get_sign(predicted_index)\n",
    "    confidence = predictions[0][predicted_index] * 100\n",
    "\n",
    "    # --- Display result on the main frame ---\n",
    "    text = f'{predicted_sign} ({confidence:.2f}%)'\n",
    "    cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # --- Show the video frame ---\n",
    "    cv2.imshow('Sign Language Recognition', frame)\n",
    "    \n",
    "    # Exit loop on 'q' press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- Cleanup ---\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
